{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjhiEjZ922AsZeyVsLvV35",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riti13/codsoft_task1/blob/main/Chatbot(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import random\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu7PRhoSYjBd",
        "outputId": "d1a66b48-6e52-48ea-d57b-ae9ad51c2017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f=open('ML.txt','r',errors='ignore')\n",
        "data=f.read()\n",
        "data=data.lower()\n",
        "sent_tokenize=nltk.sent_tokenize(data)\n",
        "wrd_tokenize=nltk.word_tokenize(data)"
      ],
      "metadata": {
        "id": "X_hevOEWYjFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMS0FjpjfvvJ",
        "outputId": "7bef4dd2-2130-4261-9a9f-950efe1315b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['machine learning (ml) is an umbrella term for solving problems for which development of algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines \"discover\" their \"own\" algorithms,[1] without needing to be explicitly told what to do by any human-developed algorithms.',\n",
              " '[2] recently, generative artificial neural networks have been able to surpass results of many previous approaches.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrd_tokenize[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNIDFgkrfvyM",
        "outputId": "5aba936b-ed2f-4f3f-d083-d4d53dba3202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['machine', 'learning']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#text pre-processing\n",
        "#punctuation\n",
        "lemmer=nltk.stem.WordNetLemmatizer()\n",
        "def lemtokens(tokens):\n",
        "  return[lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict=dict((ord(punct),None) for punct in string.punctuation)\n",
        "def lemnormalize(text):\n",
        "  return lemtokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "HyVaS4NVfv0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greet_inputs=('hello','hi','hey','sup','whats up')\n",
        "greet_outputs=['hello','hi there','how is it going','welcome','hey friend']\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greet_inputs:\n",
        "      return random.choice(greet_outputs)"
      ],
      "metadata": {
        "id": "me1QZF_hkcWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Response Generation"
      ],
      "metadata": {
        "id": "VFzrlnzYmB01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#seperate frequent using terms and rare terms\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "gxva6oSckcZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        "  robo1_response=''\n",
        "  tfidfvect=TfidfVectorizer(tokenizer=lemnormalize,stop_words='english')\n",
        "  tfidf=tfidfvect.fit_transform(sent_tokenize)\n",
        "  vals=cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx=vals.argsort()[0][-2]\n",
        "  flat=vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf=flat[-2]\n",
        "  if req_tfidf==0:\n",
        "    robo1_response=robo1_response+'I am sorry! I dont understand you.'\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response=robo1_response+sent_tokenize[idx]\n",
        "    return robo1_response"
      ],
      "metadata": {
        "id": "tMJ2zdo9mnHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining conversation start-end protocols"
      ],
      "metadata": {
        "id": "Ld_sL7Xyqfkf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "khuQIjqywtGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag=True\n",
        "print(\"BOT: Hi I am your AI assist. Lets have a conversation! Also,if you want to exit anytime, just type Bye!\")\n",
        "while(flag==True):\n",
        "  user_response=input()\n",
        "  user_response=user_response.lower()\n",
        "  if(user_response!='bye'):\n",
        "    if(user_response=='thanks'or user_response=='thank you'):\n",
        "      flag=False\n",
        "      print(\"BOT:You are Welcome.\")\n",
        "    else:\n",
        "      if(greet(user_response)!=None):\n",
        "        print(\"BOT:\",greet(user_response))\n",
        "      else:\n",
        "        sent_tokenize.append(user_response)\n",
        "        wrd_tokenize=wrd_tokenize+nltk.word_tokenize(user_response)\n",
        "        final_words=list(set(wrd_tokenize))\n",
        "        print(\"BOT:\",end='')\n",
        "        print(response(user_response))\n",
        "        sent_tokenize.remove(user_response)\n",
        "  else:\n",
        "    flag=False\n",
        "    print(\"BOT: Bye! Take Care <3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEA83Br4mnK_",
        "outputId": "abb69274-b575-40d4-ec3a-62e4cf867dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOT: Hi I am your AI assist. Lets have a conversation! Also,if you want to exit anytime, just type Bye!\n",
            "hey\n",
            "BOT: how is it going\n",
            "what is machine learning\n",
            "BOT:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14] a representative book on research into machine learning during the 1960s was nilsson's book on learning machines, dealing mostly with machine learning for pattern classification.\n",
            "renforcement learning\n",
            "BOT:learning classifier systems (lcs) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning.\n",
            "reinforcement learning\n",
            "BOT:[43]\n",
            "\n",
            "reinforcement learning\n",
            "main article: reinforcement learning\n",
            "reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward.\n",
            "bye\n",
            "BOT: Bye! Take Care <3\n"
          ]
        }
      ]
    }
  ]
}